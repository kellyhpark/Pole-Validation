{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "While the app's main functionality centers on capturing and analyzing images of utility poles for maintenance and safety concerns, the integrated text box feature serves a crucial supplementary role, it allows customers to provide additional context, report specific issues, and express their concerns or feedback regarding the utility services.\n",
    "\n",
    "On top of sentiment and urgency text analysis we will implement the following to further enhance the use of the text:\n",
    "- Support Vector Machine for Topic Classification - This model will help identify the urgency of an asset's condition by categorizing the text into 'infrastructure_and_utility_damage' or 'requests_or_urgent_needs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilizes the HumAID Twitter dataset, which consists of manually annotated tweets collected during eleven significant natural disaster events from 2016 to 2019. \n",
    "We use this HumAID Twitter dataset as a stand-in for actual customer data from SDG&E due to the current lack of available customer interactions. Our approach utilizes these annotated tweets as a proxy to develop and refine our sentiment analysis and topic classification models.\n",
    "\n",
    "Link: https://crisisnlp.qcri.org/humaid_dataset\n",
    "\n",
    "The dataset is organized into separate folders for each of the following natural disaster events:\n",
    "\n",
    "- Canada Wildfires (2016)\n",
    "- Cyclone Idai (2019)\n",
    "- Ecuador Earthquake (2016)\n",
    "- Hurricane Harvey (2017)\n",
    "- Hurricane Irma (2017)\n",
    "- Hurricane Maria (2017)\n",
    "- Hurricane Matthew (2016)\n",
    "- Italy Earthquake (August 2016)\n",
    "- Kaikoura Earthquake (2016)\n",
    "- Puebla Mexico Earthquake (2017)\n",
    "- Sri Lanka Floods (2017)\n",
    "\n",
    "For each event, the dataset is divided into two subsets: Training Set, and Test Set.\n",
    "\n",
    "Each subset consists of tab-separated values (TSV) files containing 3 columns: tweet_id, tweet_text, class_label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\",\n",
    "    \"hurricane_harvey_2017\", \"hurricane_irma_2017\", \"hurricane_maria_2017\",\n",
    "    \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\", \"kaikoura_earthquake_2016\",\n",
    "    \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"\n",
    "]\n",
    "\n",
    "dataframes = {event: {} for event in events}\n",
    "\n",
    "# Loop through each event and load train, dev, and test sets\n",
    "for event in events:\n",
    "    for set_type in ['train', 'dev', 'test']:\n",
    "        file_path = f'../data/HumAID/{event}/{event}_{set_type}.tsv'\n",
    "        # Load the dataset and store it in the dictionary under the appropriate event and set type\n",
    "        dataframes[event][set_type] = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>728674116773904384</td>\n",
       "      <td>RT @FoothillsFCU23: In response the to the #Fo...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>729787427829612544</td>\n",
       "      <td>Redcross is offering charitable donation recei...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730510385544085505</td>\n",
       "      <td>RT @globeandmail: Red Cross to transfer $50-mi...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>733705874594746368</td>\n",
       "      <td>Live: Emergency operations briefing on north A...</td>\n",
       "      <td>other_relevant_information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>730606066023665665</td>\n",
       "      <td>$9bn fire damage to Fort McMurray, ‘the beast’...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>729062171993374720</td>\n",
       "      <td>I feel sad Mom &amp;amp; I r donating money to the...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>728733841230311425</td>\n",
       "      <td>This is the best way to help. The Red Cross wi...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>730078294259961856</td>\n",
       "      <td>RT @TheGrayGroup: Donations for Fort McMurray ...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>730002964035723264</td>\n",
       "      <td>Local volleyball team members raise money to h...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>729526488366616576</td>\n",
       "      <td>These heroes brought students to their familie...</td>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>445 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                                         tweet_text  \\\n",
       "0    728674116773904384  RT @FoothillsFCU23: In response the to the #Fo...   \n",
       "1    729787427829612544  Redcross is offering charitable donation recei...   \n",
       "2    730510385544085505  RT @globeandmail: Red Cross to transfer $50-mi...   \n",
       "3    733705874594746368  Live: Emergency operations briefing on north A...   \n",
       "4    730606066023665665  $9bn fire damage to Fort McMurray, ‘the beast’...   \n",
       "..                  ...                                                ...   \n",
       "440  729062171993374720  I feel sad Mom &amp; I r donating money to the...   \n",
       "441  728733841230311425  This is the best way to help. The Red Cross wi...   \n",
       "442  730078294259961856  RT @TheGrayGroup: Donations for Fort McMurray ...   \n",
       "443  730002964035723264  Local volleyball team members raise money to h...   \n",
       "444  729526488366616576  These heroes brought students to their familie...   \n",
       "\n",
       "                                class_label  \n",
       "0    rescue_volunteering_or_donation_effort  \n",
       "1    rescue_volunteering_or_donation_effort  \n",
       "2    rescue_volunteering_or_donation_effort  \n",
       "3                other_relevant_information  \n",
       "4         infrastructure_and_utility_damage  \n",
       "..                                      ...  \n",
       "440  rescue_volunteering_or_donation_effort  \n",
       "441  rescue_volunteering_or_donation_effort  \n",
       "442  rescue_volunteering_or_donation_effort  \n",
       "443  rescue_volunteering_or_donation_effort  \n",
       "444  rescue_volunteering_or_donation_effort  \n",
       "\n",
       "[445 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['canada_wildfires_2016']['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# Iterate over each event and set type, adding the event name as a column\n",
    "for event, sets in dataframes.items():\n",
    "    for set_type, df in sets.items():\n",
    "        df['event'] = event  # Add the event name as a column\n",
    "\n",
    "        all_dfs.append(df)\n",
    "\n",
    "# Concatenate the lists of DataFrames into a single DataFrame\n",
    "all_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DataFrame that only contains the labels 'infrastructure_and_utility_damage' and 'requests_or_urgent_needs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[(all_df['class_label'] == 'infrastructure_and_utility_damage') | (all_df['class_label'] == 'requests_or_urgent_needs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>729658483948302336</td>\n",
       "      <td>Great tale from @ReutersWinnipeg: #Canada fire...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>canada_wildfires_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>731202296210685952</td>\n",
       "      <td>RT @CBCNorth: Justin Trudeau in Fort McMurray ...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>canada_wildfires_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>728697237153447936</td>\n",
       "      <td>Wildfire official Chad Morrison says between 1...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>canada_wildfires_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>733413736686358528</td>\n",
       "      <td>RT @WaterTrends: CANADA ALBERTA: Wildfire cont...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>canada_wildfires_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>728604991397584896</td>\n",
       "      <td>Catastrophic Canadian Wildfire Is a Sign of De...</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>canada_wildfires_2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                         tweet_text  \\\n",
       "16  729658483948302336  Great tale from @ReutersWinnipeg: #Canada fire...   \n",
       "21  731202296210685952  RT @CBCNorth: Justin Trudeau in Fort McMurray ...   \n",
       "32  728697237153447936  Wildfire official Chad Morrison says between 1...   \n",
       "39  733413736686358528  RT @WaterTrends: CANADA ALBERTA: Wildfire cont...   \n",
       "45  728604991397584896  Catastrophic Canadian Wildfire Is a Sign of De...   \n",
       "\n",
       "                          class_label                  event  \n",
       "16  infrastructure_and_utility_damage  canada_wildfires_2016  \n",
       "21  infrastructure_and_utility_damage  canada_wildfires_2016  \n",
       "32  infrastructure_and_utility_damage  canada_wildfires_2016  \n",
       "39  infrastructure_and_utility_damage  canada_wildfires_2016  \n",
       "45  infrastructure_and_utility_damage  canada_wildfires_2016  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id       7554\n",
       "tweet_text     7554\n",
       "class_label    7554\n",
       "event          7554\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to clean the text in our dataset, so we will create a function to remove stop words, lowercase, and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y2/69311w_s6w35j1241xp4y2lw0000gp/T/ipykernel_5862/3977374347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/streetwatch5/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/streetwatch5/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/streetwatch5/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/streetwatch5/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/y2/69311w_s6w35j1241xp4y2lw0000gp/T/ipykernel_5862/2373235469.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodified\u001b[0m \u001b[0minitial\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;31m# HTML decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lowercase text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREPLACE_BY_SPACE_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# replace REPLACE_BY_SPACE_RE symbols by space in text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/streetwatch5/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# At this point either we have a TreeBuilder instance in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "all_df['tweet_text'] = all_df['tweet_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df['tweet_text']\n",
    "y = all_df['class_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9638288486987208\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "infrastructure_and_utility_damage       0.96      0.99      0.98      1768\n",
      "         requests_or_urgent_needs       0.96      0.87      0.91       499\n",
      "\n",
      "                         accuracy                           0.96      2267\n",
      "                        macro avg       0.96      0.93      0.95      2267\n",
      "                     weighted avg       0.96      0.96      0.96      2267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=10, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "my_tags = list(all_df['class_label'].unique())\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SVM gives us an accuracy of 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['infrastructure_and_utility_damage'], dtype='<U33')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.predict(['This pole has a slight tilt to it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../models/svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(sgd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
